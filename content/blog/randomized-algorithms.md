# The randomized algorithm mindset: how can we embed machine learning seamlessly into algorithmic design?

Perhaps at the core of computer science lies the concept of algorithm: that set of instructions to be run on a Turing machine. Algorithms are solutions to all sorts of problems, from sorting numbers to efficiently selling stuff in a circuit of cities, but their general nature has not percluded their efficient classifications. When we think on algorithm classes we typically think in terms of worst case analysis, that is, given the size of the input, how many calculations does the algorithm take to finish and give us an answer in the worst possible case. This mindset pushes the algorithmic designer to generate recipes that are truly efficient no matter the input -- it is satisfying to know that sorting n numbers will always take no more than an order of nlog(n) operations. This way of thinking permeates not only algorithmic design but extends to classifying the problems these algorithms solve and gives rise to the famous P/NP classification. Unfortunately, this also puts a bit of a theoretical barrier when dealing with problems that worst-case analysis deems the hardest. Since we don't know, and it's extremely unlikely that, NP-hard problems can be solved efficiently, whenever we confront one of these problems (and boy are they everywhere) we know that we will have to compromise somehow. 

The quest for solving these difficult problems (often combinatorial in nature) have yielded (or improved) rich and very general optimization techinques such as simulated annealing, cross entropy, tabu search, genetic algorithms, and yes even deep reinforcement learning. The main idea goes like this: since many of these NP-hard problems deal with an exploding space of possibilities depending on their input size, then somehow search that space efficiently for the best solution you can find. If you're tackling the travelling salesman problem, then it becomes the space of paths within a graph. How efficiently the space is searched will depend on the nature (and sometimes instance!) of the problem and of the method you're trying: e.g. for some types of graphs, the traveling salesman problem will be better approximated with say tabu search while in others reinforcement learning will work pretty well. In other words, your algorithm will be tailored to the data you are analyzing. It might not be general enough to solve all instances of the problem, but for practical purposes it doesn't need to be.

There is another way of approximating these hard problems that is deceptively simple: just by sheer luck. Randomized algorithms are algorithms in which one step or variable is treated as a random variable. The most famous randomized algorithm is probably quicksort for sorting stuff. The reason it's so popular even though it is O(n^2) is that it is so easy to understand and to implement. Who cares that the worst case is bad, when analyzing the average case among lists of comparable items, quicksort is still just nlog(n). And that's the gist of randomized algorithms. They are typically very simple and clever, easy to understand and implement, but play a game of luck for the expected quality of the solution -- when analyzing them, you typically see statements that they approximate the solution by a factor of X "with high probability".
