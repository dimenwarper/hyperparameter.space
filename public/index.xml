<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>_index on Hyperparameter Space</title>
    <link>http://hyperparameter.space/</link>
    <description>Recent content in _index on Hyperparameter Space</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 11 Jun 2017 21:51:17 -0700</lastBuildDate>
    
	<atom:link href="http://hyperparameter.space/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Software is eating AI</title>
      <link>http://hyperparameter.space/blog/software-is-eating-ai/</link>
      <pubDate>Wed, 19 Jul 2017 10:03:29 -0700</pubDate>
      
      <guid>http://hyperparameter.space/blog/software-is-eating-ai/</guid>
      <description>There&amp;rsquo;s a famous line written by legend Marc Andreessen that summarizes the vast power of growth and disruption that commoditized computation has come to have: &amp;ldquo;Software is eating the world&amp;rdquo;. Earlier in the year, Jensen Huang from Nvidia ominously turned the phrase on its head: &amp;ldquo;Software is eating the world, but AI is going to eat software&amp;rdquo;. In many ways, I think this prophecy will indeed come to pass. Current software has become so pervasive because we have tools that translate tasks that would seem daunting (configure millions of circuit states to solve millions of repetitive tasks) into programming languages that are easy to write, learn, and teach.</description>
    </item>
    
    <item>
      <title>When not to use deep learning</title>
      <link>http://hyperparameter.space/blog/when-not-to-use-deep-learning/</link>
      <pubDate>Fri, 16 Jun 2017 21:17:31 -0700</pubDate>
      
      <guid>http://hyperparameter.space/blog/when-not-to-use-deep-learning/</guid>
      <description>I know it&amp;rsquo;s a weird way to start a blog with a negative, but there was a wave of discussion in the last few days that I think serves as a good hook for some topics on which I&amp;rsquo;ve been thinking recently. It all started with a post in the Simply Stats blog by Jeff Leek on the caveats of using deep learning in the small sample size regime. In sum, he argues that when the sample size is small (which happens a lot in the bio domain), linear models with few parameters perform better than deep nets even with a modicum of layers and hidden units.</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://hyperparameter.space/about/</link>
      <pubDate>Tue, 11 Apr 2017 16:23:34 -0700</pubDate>
      
      <guid>http://hyperparameter.space/about/</guid>
      <description>My full name is Sergio Pablo Sanchez Cordero Gonzalez but I usually go (and publish) by Pablo Cordero. I&amp;rsquo;m currently a postdoc at UCSC&amp;rsquo;s systems biology group doing applied machine learning research in the context of cell biology and regenerative medicine, particularly looking at single-cell measurements. I also do consulting from time to time. My hobbies are natural language processing and deep learning. Check out the links to my github, scholar, mail, etc.</description>
    </item>
    
  </channel>
</rss>