# The randomized algorithm mindset: how can we embed machine learning seamlessly into algorithmic design?

Perhaps at the core of computer science lies the concept of algorithm: that set of instructions to be run on a Turing machine. Algorithms are solutions to all sorts of problems, from sorting numbers to efficiently selling stuff in a circuit of cities, but their general nature has not percluded their efficient classifications. When we think on algorithm classes we typically think in terms of worst case analysis, that is, given the size of the input, how many calculations does the algorithm take to finish and give us an answer in the worst possible case. This mindset pushes the algorithmic designer to generate recipes that are truly efficient no matter the input -- it is satisfying to know that sorting n numbers will always take no more than an order of nlog(n) operations. This way of thinking permeates not only algorithmic design but extends to classifying the problems these algorithms solve and gives rise to the famous P/NP classification. Unfortunately, this also puts a bit of a theoretical barrier when dealing with problems that worst-case analysis deems the hardest. Since we don't know, and it's extremely unlikely that, NP-hard problems can be solved efficiently, whenever we are confronted by one of these problems (and boy are they everywhere) we know that we will have to compromise somehow and just do our best 

## Metaheuristics

In the quest for solving these difficult problems (often combinatorial in nature), many general frameworks have beenemployed or invented. Typically, one turns to very general optimization techinques such as simulated annealing, particle swarm optimization, cross entropy, tabu search, genetic algorithms, and, more recently, even deep reinforcement learning. These are sometimes called metaheuristics, since they are not algorithms per se, but rather recipes to create algorithms that can solve a particular task. In essence, all of them present ways of somehow searching or sampling the massive solution space efficiently to find a reasonable solution. If you're tackling the travelling salesman problem, then it becomes the space of paths within a graph. How efficiently the space is searched will depend on the nature (and sometimes instance!) of the problem and of the method you're trying: e.g. for some types of graphs, the traveling salesman problem will be better approximated with say tabu search while in others reinforcement learning will work pretty well. In other words, your algorithm will be tailored to the data you are analyzing. It might not be general enough to solve all instances of the problem, but for practical purposes it doesn't need to be. Unfortunately, this also makes the general analysis of algorithms produced by a metaheuristic more complicated and it's hard to get good theoretical guarantees. An additional complexity is that the effectiveness applying a metaheuristic will also depend on how familiarized one is with the metaheuristic. Practitioners of each metaheuristic typically have rules of thumbs (heuristics for metaheuristics!) for generating a good approximation algorithms given each situation. 

## Randomized Algorithms

There is another deceptively simple way of approximating NP-hard problems: just by sheer luck. Randomized algorithms are algorithms in which one step or variable is treated as a random variable. The most famous randomized algorithm is probably the list-sorting algorithm quicksort. Just randomly choose a random pivot on the list, divide, and repeat. The reason it's so popular even though it is O(n^2) is that it is so easy to understand and to implement. Who cares that the worst case is bad, when analyzing the average case among lists of comparable items, quicksort is still just nlog(n). That's the gist of randomized algorithms. They are typically very simple and clever, easy to understand and implement, but play a game of luck for the expected quality of the solution -- when analyzing them, you typically see statements that they approximate the optimal solution by a factor of X "with high probability". 

Note that most (all?) of the algorithms produced through metaheuristics are in fact randomized algorithms by definition since they have to sample the solution space somehow. However, at least to my appreciation, the algorithms produced by the randomized algorithms field are different in spirit: they are constructs that try to be more general and give good theoretical guarantees (or in some cases tight approximation factor bounds). They are simple and well understood, but their solutions are typically inferior to well tuned metaheuristic instances that are massively sampled. Keeping this in mind, I will refer to randomized algorithms as algorithms coming from that field, which is something separate to their metaheuristic kin.

## Best of both worlds

What I love about randomized algorithms is how straightforward they are to implement. What I love about metaheuristics is their flexibility that allows them to be tuned to my particular dataset. Is there a way to bring the two together? 
