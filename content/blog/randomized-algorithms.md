# The randomized algorithm mindset: how can we embed machine learning seamlessly into algorithmic design?

Given the successes of deep learning frameworks in particular and of machine learning in general, there has been a lot of excitement lately on proposals for reinventing the way we do software even to the point where one of the godfathers of deep learning wants to rename the field to differential programming. Indeed, why tell the computer exactly what to do? Just give it examples of what you want to do, set up a model, and let machine learning figure out the rest. For many human-like tasks, such as question and answering or summarizing text, this is the best way we have right now to do this since there's no universal set of instructions that can solve the task in every context. But there are other tasks that do have very general and efficient algorithms that would, at first glance, seem silly to code with machine learning. Why train a neural network to sort numbers, or to find the maximum flow in the graph, or to solve a linear program where a real neural network (that of a human) already codified it into a pretty general recipe that works well and fast? In this post, I argue that even those tasks can benefit of machine learning models, that is, there is much to be gained by embedding machine learning models into algorithmic design and implementation. In particular, tailoring algorithmic implementations to the data that they will be used on will likely result in performance improvement -- e.g. even a simple sorting algorithm can benefit knowing, beforehand, that half of the list is already sorted. This way of conceptualizing the coding of algorithms leads to all sorts of interesting consequences: coding primarily as a means to get data, problem complexity as a function of data, compiling the algorithm structure itself, and the need to embed data into package management. This viewpoint is best illustrated when dealing with NP-hard problems and is, serendipitously, best exemplified  by one strategy to tackle these tough problems: randomized algorithms. To ground these ideas, I'll introduce a coding framework, called learnlets, for embedding ML into algorithmic design and a python proof-of-concept implementation that helps illustrate one way of getting to software 2.0.

## The toughest problems

Perhaps at the core of computer science lies the concept of algorithm: that set of instructions to be run on a Turing machine. Algorithms are solutions to all sorts of problems, from sorting numbers to efficiently selling stuff in a circuit of cities, but their general nature has not percluded their efficient classifications. When we think on algorithm classes we typically think in terms of worst case analysis, that is, given the size of the input, how many calculations does the algorithm take to finish and give us an answer in the worst possible case. This mindset pushes the algorithmic designer to generate recipes that are truly efficient no matter the input and come with a guaranteed running time -- it is satisfying to know that sorting n numbers will always take no more than an order of nlog(n) operations. This way of thinking permeates not only algorithmic design but extends to classifying the problems these algorithms solve and gives rise to the famous P/NP classification. Unfortunately, this also puts a bit of a psychological  barrier when dealing with problems that worst-case analysis deems the hardest. Since we don't know, and it's extremely unlikely that, NP-hard problems can be solved efficiently, whenever we are confronted by one of these problems (and boy are they everywhere) we know that we will have to compromise somehow and just do our best to find an approximate solution. 

## Metaheuristics

In the quest for solving these difficult problems (often combinatorial in nature), many general frameworks have beenemployed or invented. Typically, one turns to very general optimization techinques such as simulated annealing, particle swarm optimization, cross entropy, tabu search, genetic algorithms, and, more recently, even deep reinforcement learning. These are sometimes called metaheuristics, since they are not algorithms per se, but rather recipes to create algorithms that can solve a particular task. In essence, all of them present ways of somehow searching or sampling the massive solution space efficiently to find a reasonable solution. If you're tackling the travelling salesman problem, then it becomes the space of paths within a graph. How efficiently the space is searched will depend on the nature (and sometimes instance!) of the problem and of the method you're trying: e.g. for some types of graphs, the traveling salesman problem will be better approximated with say tabu search while in others reinforcement learning will work pretty well. In other words, your algorithm will be tailored to the particular instance you are analyzing. It might not be general enough to solve all instances of the problem, but for practical purposes it doesn't need to be. Unfortunately, this also makes the general analysis of algorithms produced by a metaheuristic more complicated and it's hard to get good theoretical guarantees. An additional complexity is that the effectiveness applying a metaheuristic will also depend on how familiarized one is with the metaheuristic. Practitioners of each metaheuristic typically have rules of thumbs (heuristics for metaheuristics!) for generating a good approximation algorithms given each situation. 

## Randomized Algorithms

There is another deceptively simple way of approximating NP-hard problems: just by sheer luck. Randomized algorithms are algorithms in which one step or variable is treated as a random variable. The most famous randomized algorithm is probably the list-sorting algorithm quicksort. Just randomly choose a random pivot on the list, divide, and repeat. The reason it's so popular even though it is O(n^2) is that it is so easy to understand and to implement. Who cares that the worst case is bad, when analyzing the average case among lists of comparable items, quicksort is still just nlog(n). That's the gist of randomized algorithms. They are typically very simple and clever, easy to understand and implement, but play a game of luck for the expected quality of the solution -- when analyzing them, you typically see statements that they approximate the optimal solution by a factor of X "with high probability". 

Note that most (all?) of the algorithms produced through metaheuristics are in fact randomized algorithms by definition since they have to sample the solution space somehow. However, at least to my appreciation, the algorithms produced by the randomized algorithms field are different in spirit: they are constructs that try to be more general and give good theoretical guarantees (or in some cases tight approximation factor bounds). They are simple and well understood, but their solutions are typically inferior to well tuned metaheuristic instances that are massively sampled. Keeping this in mind, I will refer to randomized algorithms as algorithms coming from that field, which is something separate to their metaheuristic kin.

## Learnlets: smart de-randomization using embedded ML

What I love about randomized algorithms is how straightforward they are to implement. What I love about metaheuristics is their flexibility that allows them to be tuned to my particular dataset. Is there a way to bring the two together? I think a good answer lies in the old adage: "implement now, optimize later". My ideal workflow would go like this: 

1. For a given NP-hard problem you code up a super simple randomized algorithm that produces a (very crude) approximate solution by taking random decisions at critical points where some heavy, exponential computation would otherwise be required.
2. You somehow flag those random, critical decision points to be optimized later.
3. You go about your way using the algorithm, getting a sense of what data is it most commonly going to be used on.
4. At some point, after collecting or generating data that your algorithm is being applied on, the system optimizes the flagged critical decision points, tailoring it to your data, using a metaheuristic. The random decisions points are gone, you have effectively de-randomized your algorithm.
5. The system continuously learns as you keep calling your algorithm with new data, optimizing the decision points periodically as new data comes in.

In the end, you would have coded up a 'dumb' algorithm that gets better (for your particular task at least) as you keep using it. For lack of a better term, I'll be calling these trainable randomized algorithms *learnlets*, pieces of code with a bit of embedded machine learning that learn as they go, and the critical decision points that will be learned/optimized as *learnables*.

## A python example: the MAXCUT problem

To ground the concept, let's look at an example. Consider the MAXCUT problem: given a weighted graph, find an optimal way to split it into two subgraphs such that the sum of edge weights between subgraphs is maximum. MAXCUT is NP-hard and is typically used as one of the first examples for illustrating randomized algorithms. The naive randomized algorithm for MAXCUT is extremely simple: for each vertex, randomly choose if it will be part of one subgraph or the other. A simple analysis reveals that this algorithm gives an expected cut of at least half the amount of the actual maximum cut, so in a way it gets us halfway there. This is the python implementation:


Notice how I'm using the graph vertex degrees as input to the random decision function `vertex_set_probs`, even though I don't use them at all. This is intentional, I'm thinking ahead in that these will be the features that will be used to better the improve the algorithm. It also outputs probabilities of sets, which will be useful for scoring solutions, we can at anytime threshold these proabilities at 0.5 to decide which vertex belongs to which set. Now, what I want to do is somehow turn this into a learnlet, by flagging the `vertex_set_probs` function as a learnable decision function. To this end, I've implemented a very simple learnlet framework in python that uses a crude reinforcement learning strategy using policy gradients to optimize the learnables given some score function, leveraging the excellent autograd package. Inspired by the awesome numba package, my framework uses decorators to flag functions as learnets or learnables, which is a nice way to seamlessly add functionality to your code:



Some explanations of the arguments of the decorators are in order. The learnable decorator specifies that the function will be approximated with a particular predictor, in this case a very simple dense neural net. The learnlet decorator simply flags the function as a learnlet and lists all of the learnables that it uses. Notice that the first (input) layer of the `vertex_set_probs` predictor neural net is set to 20. Since the function takes an array of vertex degrees, this means that our current learnlet will only work with graphs with 20 vertices. 

Now, let's generate some data. Suppose that this code will mostly run on graphs of 20 vertices that have two cliques chosen at random whith sparse, inter-clique connections. Further, suppose that the cliques have weak intra-clique connections but very strong inter-clique connections, so the max cut is obvious. Here's the code for generating these graphs and an example adjacency matrix.


I'll generate some training and testing graphs for our learnlet.

Next, I have to write a function that scores my solutions, so here it is.


So the score function is basically the expected value of the cut given the membership probabilities. Why didn't I use the more straightforward score that thresholds the probabilities and simply takes the value of the resulting cut? Turns out that this results in a difficult-to-optimize function that varies step-wise (small changes in membership probability will yield the same cut) and so when doing gradient descent you end up with very flat regions of no change that mess up the calculations.

Alright, we're ready to optimize our learnlet!


The `optimize` function takes typical optimization hyperparameters to perform gradient descent on neural nets and then optimizes the predictors for each learnable in the specified learnlet. Let's run our learnlet with and without using the predictors in the train/test data.


Notice the `predict=True` argument when running the max cut function. This is an extra keyword argument that is added by the learnlet decorator that switches the learnlet to prediction mode, in which it uses the learnable predictors instead of the hard-coded, random function to execute the algorithm. Let's now compare the results of our random baseline with our de-randomized learnlet as well to the "ground truth".


Cool! Something definitely happened, the score is up in both the train and test sets, so our learnlet did learn something. It didn't quite pick up the optimal value which is a shame, although that would be difficult using only vertex degree information. For this simple problem, a convolutional neural net that takes all of the adjacency matrix would probably find the right solution. Still, we basically got this performance boost for free, just adding a couple of decorators to our functions. We can confirm that our learned solutions are better than baseline using the standard max cut score instead of the expected score:


The current learnlet approach still needs improving in several areas. For example, the hyperparameters I chose for the neural net predictor and the optimizer required a bit of experimentation and is something that could be totally automated away using auto ML techniques. Other optimization metaheuristics could be implemented as well, and parallelization of the learning routines, as well as using more scalable machine learning frameworks like PyTorch, Tensorflow, would be desirable. 

I've also experimented with adding ways for automatically capturing the training data of a learnlet: e.g. each time a learnlet-decorated function is called, save the arguments in some database and then use the aggregated data to train the learnlet. For the testing purposes above, I found that this was getting in the way as I needed to measure performance with test/train sets. However, in the real world and when using the algorithm, one could see that a data-capturing feature would be necessary in order to understand the data context where the code is being used.  

## Implications of embedding ML

What are the implications of this way of coding algorithms? Here are some that I think are interesting. 

### Algorithm compiling

For decades, we've used compilers to optimize the code we write, translating it to efficient machine code that rearranges our loops, conditionals, etc. so that it runs faster than simply transcribing our instructions as-is to op codes. The instructions are changed by the compiler, but the essence of the algorithm remains the same. But how about "compiling" the structure of the algorithm itself, optimizing not only its speed but the quality of its solutions given our data? I think this is a task that is well-suited for a combination of machine learning with traditional programming since we can seamlessly combine the two to get better results with little effort, like in the learnlet framework described above. In a way, in this "algorithm compiling" workflow, one uses an easily-implemented but dumbed-down algorithm as an instrument to quickly capture the data context of the problem and that serves as minimal control logic, where the heavy stuff is handed down to the embedded ML.

### P vs NP, whithin the context of your data

In hindsight, it seems surprising that it's not standard practice to record (or at least record periodically) the data each function in our codebase is being called on as it pretty much defines the worlds our programs live in. It also defines how efficient and effective they are going to be and even what unit tests should be written. No algorithm is going to be run on all the possible inputs and in most cases the worst-case input simply doesn't happen. Consequently, and loosely speaking, the definitions of P/NP change between universes of data. In some data contexts, the travelling salesman problem could be optimally approximated by a greedy algorithm. Learning these best approximations automatically within the data at hand is where ML really shines.

### Package management, with data

Say you coded up a learnlet and trained it on your data. Say your data is general enough that some other folks might also have similar data and could use the same optimized learnlet. How do you share your trained learnlet with them? Is it part of your package when you deploy in your favority package management system? But what if your data is private? These are tough questions, but we may already have the technology to answer them. There could be repositories of learned models that you could run your data against, securely, to see if it matches, and then get an already optimized learnlet as your starting point. Performing these checks securely is something than could be done in e.g. a blockchain, similarly as how openmined trains models in a trustless, distributed setting -- so package management, put it on the blockchain!

## Opportunities galore

Using ML as a main driver when coding presents both huge opportunities and challenges. Some steps required to get good models and therefore good programs are still inefficient. For example, performing Auto ML to find the best model is still very time-consuming and many times resource hungry, so if you want to 'compile your algorithm' like mentioned above you might have to do it in the cloud. Debugging could be a nightmare: how do you know that your trained mdoel is failing you because of the data itself or because of some missing data preprocessing step before you trained your model?  Recording all the data that your program uses might be unfeasible, it might be simply too much data. In this case, thinking of compression and coding strategies to just save the data that best describes your data distribution could be helpful. 

These are only ideas in a brainstorm, implementing and testing them will take time, expertise, and diligence. But the investment will be worth it, in the end, the whole practice of programming will be reduced to writing data collection and control logic for running smart modules that learn to perform well within your data universe. And who knows, even that part might be learnable as well.
